"""
DemonstraÃ§Ã£o Completa do Sistema de Teste e Refinamento de Prompts
Exemplo prÃ¡tico mostrando todas as funcionalidades implementadas na Tarefa 3.4
"""

import asyncio
import aiohttp
import json
import time
from datetime import datetime
from typing import Dict, Any, List

# URLs da API (ajuste conforme necessÃ¡rio)
API_BASE_URL = "http://localhost:8000/api/v1"
AUTH_TOKEN = "your-jwt-token-here"  # Substitua por um token vÃ¡lido

class PromptTestingClient:
    """Cliente para interagir com a API de teste de prompts"""
    
    def __init__(self, base_url: str, auth_token: str):
        self.base_url = base_url
        self.headers = {
            "Authorization": f"Bearer {auth_token}",
            "Content-Type": "application/json"
        }
    
    async def create_ab_test(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """Cria um novo teste A/B"""
        async with aiohttp.ClientSession() as session:
            async with session.post(
                f"{self.base_url}/prompt-testing/tests/ab-test",
                headers=self.headers,
                json=config
            ) as response:
                return await response.json()
    
    async def create_iterative_test(self, base_prompt: str, target_metric: str = "image_quality") -> Dict[str, Any]:
        """Cria teste de refinamento iterativo"""
        payload = {
            "base_prompt": base_prompt,
            "target_metric": target_metric,
            "iterations": 5
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.post(
                f"{self.base_url}/prompt-testing/tests/iterative",
                headers=self.headers,
                json=payload
            ) as response:
                return await response.json()
    
    async def create_multivariate_test(self, base_prompt: str) -> Dict[str, Any]:
        """Cria teste multivariÃ¡vel"""
        payload = {
            "base_prompt": base_prompt,
            "style_options": ["realistic", "artistic", "cinematic"],
            "quality_options": ["standard", "hd"],
            "size_options": ["512x512", "1024x1024"]
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.post(
                f"{self.base_url}/prompt-testing/tests/multivariate",
                headers=self.headers,
                json=payload
            ) as response:
                return await response.json()
    
    async def run_test_iteration(self, test_id: str) -> Dict[str, Any]:
        """Executa uma iteraÃ§Ã£o de teste"""
        async with aiohttp.ClientSession() as session:
            async with session.post(
                f"{self.base_url}/prompt-testing/tests/{test_id}/run-iteration",
                headers=self.headers
            ) as response:
                return await response.json()
    
    async def get_test_status(self, test_id: str) -> Dict[str, Any]:
        """ObtÃ©m status de um teste"""
        async with aiohttp.ClientSession() as session:
            async with session.get(
                f"{self.base_url}/prompt-testing/tests/{test_id}/status",
                headers=self.headers
            ) as response:
                return await response.json()
    
    async def analyze_test(self, test_id: str) -> Dict[str, Any]:
        """Analisa resultados de um teste"""
        async with aiohttp.ClientSession() as session:
            async with session.get(
                f"{self.base_url}/prompt-testing/tests/{test_id}/analysis",
                headers=self.headers
            ) as response:
                return await response.json()
    
    async def get_optimization_suggestions(self, prompt: str) -> List[Dict[str, Any]]:
        """ObtÃ©m sugestÃµes de otimizaÃ§Ã£o para um prompt"""
        # SimulaÃ§Ã£o - em produÃ§Ã£o seria um endpoint real
        return [
            {
                "type": "quality_enhancement",
                "description": "Adicionar palavras-chave de qualidade",
                "example": f"{prompt}, high quality, detailed",
                "expected_improvement": 15
            },
            {
                "type": "style_definition", 
                "description": "Definir estilo artÃ­stico",
                "example": f"{prompt}, digital art style",
                "expected_improvement": 10
            }
        ]
    
    async def auto_optimize_batch(self, prompts: List[str]) -> Dict[str, Any]:
        """Otimiza prompts automaticamente em lote"""
        payload = prompts
        
        async with aiohttp.ClientSession() as session:
            async with session.post(
                f"{self.base_url}/prompt-testing/batch/auto-optimize",
                headers=self.headers,
                json=payload
            ) as response:
                return await response.json()
    
    async def get_testing_statistics(self) -> Dict[str, Any]:
        """ObtÃ©m estatÃ­sticas de testes"""
        async with aiohttp.ClientSession() as session:
            async with session.get(
                f"{self.base_url}/prompt-testing/statistics",
                headers=self.headers
            ) as response:
                return await response.json()


async def demo_ab_test():
    """DemonstraÃ§Ã£o de teste A/B simples"""
    print("ğŸ¯ === DEMO: Teste A/B de Prompts ===")
    
    client = PromptTestingClient(API_BASE_URL, AUTH_TOKEN)
    
    # 1. Configurar teste A/B
    print("\n1. Configurando teste A/B...")
    
    test_config = {
        "test_type": "ab_test",
        "base_prompt": "a beautiful robot in a garden",
        "variants": [
            {
                "id": "variant_a",
                "prompt": "a beautiful robot in a garden",
                "style_modifiers": [],
                "technical_params": {"quality": "standard"},
                "description": "Prompt original bÃ¡sico"
            },
            {
                "id": "variant_b",
                "prompt": "a beautiful robot in a garden, high quality, detailed, artistic masterpiece",
                "style_modifiers": ["artistic", "detailed"],
                "technical_params": {"quality": "hd"},
                "description": "Prompt otimizado com qualidade"
            },
            {
                "id": "variant_c",
                "prompt": "a beautiful futuristic robot in a lush cyberpunk garden, cinematic lighting, 8k, photorealistic",
                "style_modifiers": ["cinematic", "futuristic", "photorealistic"],
                "technical_params": {"quality": "hd", "resolution": "8k"},
                "description": "Prompt avanÃ§ado com elementos cinematogrÃ¡ficos"
            }
        ],
        "target_metrics": ["image_quality", "aesthetic_score", "generation_time"],
        "sample_size": 15  # 5 amostras por variante
    }
    
    try:
        test_result = await client.create_ab_test(test_config)
        test_id = test_result["test_id"]
        print(f"   âœ… Teste criado: {test_id}")
        print(f"   ğŸ“Š Variantes: {test_result['variants_count']}")
        print(f"   ğŸ¯ Meta de amostras: {test_result['target_sample_size']}")
        
        # 2. Executar iteraÃ§Ãµes do teste
        print(f"\n2. Executando iteraÃ§Ãµes do teste...")
        
        for i in range(test_config["sample_size"]):
            try:
                iteration_result = await client.run_test_iteration(test_id)
                
                variant_tested = iteration_result["variant_tested"]
                total_samples = iteration_result["total_samples"]
                target_samples = iteration_result["target_samples"]
                
                print(f"   IteraÃ§Ã£o {i+1}: Testou {variant_tested} | "
                      f"Progresso: {total_samples}/{target_samples}")
                
                # Simula intervalo entre testes
                await asyncio.sleep(0.5)
                
                # Verifica se teste foi concluÃ­do
                if iteration_result.get("analysis"):
                    print(f"   ğŸ† Vencedor declarado automaticamente!")
                    break
                    
            except Exception as e:
                print(f"   âš ï¸ Erro na iteraÃ§Ã£o {i+1}: {e}")
                continue
        
        # 3. Analisar resultados
        print(f"\n3. Analisando resultados finais...")
        
        try:
            analysis = await client.analyze_test(test_id)
            
            print(f"   ğŸ† Variante vencedora: {analysis['winner_variant_id']}")
            print(f"   ğŸ“ˆ Melhoria: {analysis['improvement_percentage']:.1f}%")
            print(f"   ğŸ¯ ConfianÃ§a: {analysis['confidence_score']*100:.1f}%")
            print(f"   ğŸ“Š SignificÃ¢ncia estatÃ­stica: {'âœ… Sim' if analysis['statistical_significance'] else 'âŒ NÃ£o'}")
            
            print(f"\n   ğŸ” AnÃ¡lise por mÃ©trica:")
            for metric, data in analysis["metrics_analysis"].items():
                best_variant = data["best_variant"]
                improvement = data["improvement_percentage"]
                print(f"      {metric}: {best_variant} (+{improvement:.1f}%)")
            
            print(f"\n   ğŸ’¡ RecomendaÃ§Ãµes:")
            for i, rec in enumerate(analysis["recommendations"][:3], 1):
                print(f"      {i}. {rec}")
            
            print(f"\n   âœ¨ Prompt otimizado final:")
            print(f"      '{analysis['optimal_prompt']}'")
            
        except Exception as e:
            print(f"   âŒ Erro na anÃ¡lise: {e}")
        
        return test_id
        
    except Exception as e:
        print(f"âŒ Erro no teste A/B: {e}")
        return None


async def demo_iterative_refinement():
    """DemonstraÃ§Ã£o de refinamento iterativo"""
    print("\nğŸ”„ === DEMO: Refinamento Iterativo ===")
    
    client = PromptTestingClient(API_BASE_URL, AUTH_TOKEN)
    
    # 1. Criar teste iterativo
    print("\n1. Iniciando refinamento iterativo...")
    
    base_prompt = "a cat sitting on a chair"
    
    try:
        test_result = await client.create_iterative_test(base_prompt, "image_quality")
        test_id = test_result["test_id"]
        
        print(f"   âœ… Teste iterativo criado: {test_id}")
        print(f"   ğŸ“ Prompt base: '{base_prompt}'")
        print(f"   ğŸ¯ MÃ©trica alvo: {test_result['target_metric']}")
        print(f"   ğŸ”„ IteraÃ§Ãµes: {test_result['iterations']}")
        
        # 2. Executar iteraÃ§Ãµes
        print(f"\n2. Executando refinamento...")
        
        iteration_count = 0
        while iteration_count < 10:  # MÃ¡ximo 10 iteraÃ§Ãµes
            try:
                result = await client.run_test_iteration(test_id)
                iteration_count += 1
                
                variant = result["variant_tested"]
                progress = result["total_samples"]
                target = result["target_samples"]
                
                print(f"   IteraÃ§Ã£o {iteration_count}: {variant} | {progress}/{target}")
                
                if progress >= target:
                    print(f"   âœ… Refinamento completo!")
                    break
                
                await asyncio.sleep(0.3)
                
            except Exception as e:
                print(f"   âš ï¸ Erro na iteraÃ§Ã£o: {e}")
                break
        
        # 3. AnÃ¡lise do refinamento
        print(f"\n3. Analisando refinamento...")
        
        try:
            analysis = await client.analyze_test(test_id)
            
            print(f"   ğŸ¯ Melhor iteraÃ§Ã£o: {analysis['winner_variant_id']}")
            print(f"   ğŸ“ˆ Melhoria obtida: {analysis['improvement_percentage']:.1f}%")
            print(f"   âœ¨ Resultado final: '{analysis['optimal_prompt']}'")
            
        except Exception as e:
            print(f"   âŒ Erro na anÃ¡lise: {e}")
        
        return test_id
        
    except Exception as e:
        print(f"âŒ Erro no refinamento iterativo: {e}")
        return None


async def demo_multivariate_test():
    """DemonstraÃ§Ã£o de teste multivariÃ¡vel"""
    print("\nğŸ² === DEMO: Teste MultivariÃ¡vel ===")
    
    client = PromptTestingClient(API_BASE_URL, AUTH_TOKEN)
    
    # 1. Criar teste multivariÃ¡vel
    print("\n1. Configurando teste multivariÃ¡vel...")
    
    base_prompt = "a dragon flying over mountains"
    
    try:
        test_result = await client.create_multivariate_test(base_prompt)
        test_id = test_result["test_id"]
        
        print(f"   âœ… Teste multivariÃ¡vel criado: {test_id}")
        print(f"   ğŸ“ Prompt base: '{base_prompt}'")
        print(f"   ğŸ¨ Estilos: {test_result['style_options']}")
        print(f"   ğŸ”§ Qualidades: {test_result['quality_options']}")
        print(f"   ğŸ“ Tamanhos: {test_result['size_options']}")
        print(f"   ğŸ§® Total de combinaÃ§Ãµes: {test_result['total_combinations']}")
        
        # 2. Executar testes das combinaÃ§Ãµes
        print(f"\n2. Testando combinaÃ§Ãµes...")
        
        target_iterations = test_result['total_combinations'] * 2  # 2 por combinaÃ§Ã£o
        
        for i in range(target_iterations):
            try:
                result = await client.run_test_iteration(test_id)
                
                variant = result["variant_tested"]
                progress = result["total_samples"]
                
                print(f"   Teste {i+1}: {variant} | Total: {progress}")
                
                if result.get("analysis"):
                    print(f"   ğŸ† AnÃ¡lise automÃ¡tica disponÃ­vel!")
                    break
                
                await asyncio.sleep(0.2)
                
            except Exception as e:
                print(f"   âš ï¸ Erro no teste {i+1}: {e}")
                continue
        
        # 3. Status final
        print(f"\n3. Status final do teste multivariÃ¡vel...")
        
        try:
            status = await client.get_test_status(test_id)
            
            print(f"   ğŸ“Š Total de amostras: {status['total_samples']}")
            print(f"   ğŸ Teste completo: {'âœ… Sim' if status['is_complete'] else 'âŒ NÃ£o'}")
            
            print(f"\n   ğŸ“ˆ Amostras por variante:")
            for variant_id, count in status["samples_by_variant"].items():
                print(f"      {variant_id}: {count} amostras")
            
            # AnÃ¡lise se houver dados suficientes
            if status["total_samples"] >= 10:
                try:
                    analysis = await client.analyze_test(test_id)
                    print(f"\n   ğŸ† Melhor combinaÃ§Ã£o: {analysis['winner_variant_id']}")
                    print(f"   âœ¨ Prompt otimizado: '{analysis['optimal_prompt']}'")
                except:
                    print(f"   â³ AnÃ¡lise ainda em processamento...")
            
        except Exception as e:
            print(f"   âŒ Erro no status: {e}")
        
        return test_id
        
    except Exception as e:
        print(f"âŒ Erro no teste multivariÃ¡vel: {e}")
        return None


async def demo_optimization_suggestions():
    """DemonstraÃ§Ã£o de sugestÃµes de otimizaÃ§Ã£o"""
    print("\nğŸ’¡ === DEMO: SugestÃµes de OtimizaÃ§Ã£o ===")
    
    client = PromptTestingClient(API_BASE_URL, AUTH_TOKEN)
    
    # Prompts de exemplo para anÃ¡lise
    test_prompts = [
        "a cat",
        "beautiful woman",
        "car on road",
        "a detailed portrait of an elegant woman with flowing hair, professional lighting, high quality, artistic masterpiece",
        "cyberpunk city at night, neon lights, rain, cinematic, 8k, photorealistic"
    ]
    
    print("\n1. Analisando prompts e gerando sugestÃµes...")
    
    for i, prompt in enumerate(test_prompts, 1):
        print(f"\n   Prompt {i}: '{prompt}'")
        
        try:
            suggestions = await client.get_optimization_suggestions(prompt)
            
            if suggestions:
                for j, suggestion in enumerate(suggestions, 1):
                    print(f"      {j}. {suggestion['description']}")
                    print(f"         Exemplo: '{suggestion['example']}'")
                    print(f"         Melhoria esperada: +{suggestion['expected_improvement']}%")
            else:
                print(f"      âœ… Prompt jÃ¡ otimizado!")
                
        except Exception as e:
            print(f"      âŒ Erro: {e}")


async def demo_batch_optimization():
    """DemonstraÃ§Ã£o de otimizaÃ§Ã£o em lote"""
    print("\nâš¡ === DEMO: OtimizaÃ§Ã£o em Lote ===")
    
    client = PromptTestingClient(API_BASE_URL, AUTH_TOKEN)
    
    # Lista de prompts para otimizar
    prompts_to_optimize = [
        "a robot",
        "mountain landscape", 
        "portrait of a person",
        "futuristic city",
        "abstract art"
    ]
    
    print(f"\n1. Otimizando {len(prompts_to_optimize)} prompts automaticamente...")
    
    try:
        result = await client.auto_optimize_batch(prompts_to_optimize)
        
        print(f"   âœ… Lote criado: {result['batch_id']}")
        print(f"   ğŸ“Š Prompts processados: {result['prompts_count']}")
        print(f"   ğŸ¯ MÃ©trica alvo: {result['target_metric']}")
        print(f"   â±ï¸ Tempo estimado: {result['estimated_completion']}")
        
        print(f"\n2. Resultados da otimizaÃ§Ã£o:")
        for i, optimization in enumerate(result["results"], 1):
            print(f"   {i}. Original: '{optimization['original_prompt']}'")
            print(f"      Test ID: {optimization['test_id']}")
            print(f"      Status: {optimization['status']}")
        
    except Exception as e:
        print(f"âŒ Erro na otimizaÃ§Ã£o em lote: {e}")


async def demo_statistics_dashboard():
    """DemonstraÃ§Ã£o do dashboard de estatÃ­sticas"""
    print("\nğŸ“Š === DEMO: Dashboard de EstatÃ­sticas ===")
    
    client = PromptTestingClient(API_BASE_URL, AUTH_TOKEN)
    
    print("\n1. Coletando estatÃ­sticas de testes...")
    
    try:
        stats = await client.get_testing_statistics()
        
        print(f"   ğŸ‘¤ UsuÃ¡rio: {stats['user_id']}")
        print(f"   ğŸ“ Total de testes: {stats['total_tests']}")
        print(f"   ğŸ”„ Total de resultados: {stats['total_results']}")
        print(f"   ğŸ“ˆ MÃ©dia de resultados por teste: {stats['avg_results_per_test']:.1f}")
        
        print(f"\n   ğŸ“Š Tipos de teste executados:")
        for test_type, count in stats.get("test_types", {}).items():
            print(f"      {test_type}: {count} testes")
        
        print(f"\n   â° Ãšltima atividade: {stats['last_activity']}")
        
    except Exception as e:
        print(f"âŒ Erro nas estatÃ­sticas: {e}")


async def demo_complete_workflow():
    """DemonstraÃ§Ã£o do fluxo completo de trabalho"""
    print("\nğŸš€ === DEMO: Fluxo Completo de OtimizaÃ§Ã£o ===")
    
    print("Este demo mostra um fluxo tÃ­pico de otimizaÃ§Ã£o de prompts:")
    print("1. AnÃ¡lise inicial do prompt")
    print("2. Teste A/B com variantes")
    print("3. Refinamento iterativo do vencedor")
    print("4. ValidaÃ§Ã£o final")
    
    # 1. Teste A/B inicial
    ab_test_id = await demo_ab_test()
    
    if ab_test_id:
        # 2. Refinamento iterativo baseado no resultado
        await demo_iterative_refinement()
    
    # 3. AnÃ¡lise de sugestÃµes
    await demo_optimization_suggestions()
    
    # 4. Dashboard final
    await demo_statistics_dashboard()
    
    print("\nğŸ‰ Fluxo completo demonstrado!")


async def main():
    """Executa todas as demonstraÃ§Ãµes"""
    print("ğŸ§ª Sistema de Teste e Refinamento de Prompts - VideoAI")
    print("=" * 60)
    print("DemonstraÃ§Ãµes da Tarefa 3.4 - ImplementaÃ§Ã£o Completa")
    print("=" * 60)
    
    try:
        print("\nğŸ“‹ Menu de DemonstraÃ§Ãµes:")
        print("1. Teste A/B simples")
        print("2. Refinamento iterativo")
        print("3. Teste multivariÃ¡vel")
        print("4. SugestÃµes de otimizaÃ§Ã£o")
        print("5. OtimizaÃ§Ã£o em lote")
        print("6. Dashboard de estatÃ­sticas")
        print("7. Fluxo completo")
        
        # Para demo, executa todas as demonstraÃ§Ãµes
        print("\nğŸ¬ Executando todas as demonstraÃ§Ãµes...")
        
        await demo_ab_test()
        await demo_iterative_refinement()
        await demo_multivariate_test()
        await demo_optimization_suggestions()
        await demo_batch_optimization()
        await demo_statistics_dashboard()
        
        print("\n" + "=" * 60)
        print("âœ… Todas as demonstraÃ§Ãµes concluÃ­das com sucesso!")
        print("\nğŸ’¡ Funcionalidades demonstradas:")
        print("   ğŸ¯ Testes A/B automatizados")
        print("   ğŸ”„ Refinamento iterativo inteligente")
        print("   ğŸ² Testes multivariÃ¡veis completos")
        print("   ğŸ’¡ SugestÃµes de otimizaÃ§Ã£o baseadas em IA")
        print("   âš¡ OtimizaÃ§Ã£o em lote eficiente")
        print("   ğŸ“Š Analytics e estatÃ­sticas detalhadas")
        print("   ğŸ§  Aprendizado automÃ¡tico de padrÃµes")
        
    except Exception as e:
        print(f"âŒ Erro geral: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    print("ğŸ“– InstruÃ§Ãµes de Uso:")
    print("1. Certifique-se que a API estÃ¡ rodando em localhost:8000")
    print("2. Substitua AUTH_TOKEN por um token JWT vÃ¡lido")
    print("3. Execute: python prompt_testing_demo.py")
    print()
    
    # Executa as demonstraÃ§Ãµes
    asyncio.run(main()) 